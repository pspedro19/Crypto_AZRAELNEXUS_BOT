{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import fireducks.pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_mongodb(client, collection: str = '', query_json: dict = {}, query_project: dict = {}, skip: int = 0, limit: int = 0):\n",
    "    # Define MongoDB parameters\n",
    "    mongo_srv_uri = 'mongodb://localhost:27017/'\n",
    "    mongo_user = ''\n",
    "    mongo_password = ''\n",
    "    mongo_db_name = 'facsat2'\n",
    "    mongo_collection_name = collection\n",
    "\n",
    "    try:\n",
    "        # Access the MongoDB database\n",
    "        db = client[mongo_db_name]\n",
    "        collection = db[mongo_collection_name]\n",
    "\n",
    "        # Perform MongoDB operations\n",
    "        result = collection.find(query_json, query_project).skip(skip).limit(limit) \n",
    "    except Exception as e:\n",
    "        print(\"No fue posible realizar la consulta\\nError:\\n\")\n",
    "        print(e)\n",
    "    \n",
    "    data = list(result) # pl.DataFrame(list(result))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def column_summary(df: pd.DataFrame, system, node, tabla, data_info):\n",
    "    summary_data = []\n",
    "    # tabla = df[\"Tabla\"].unique()[0]\n",
    "    df.drop(columns=[\"Tabla\"], inplace=True)\n",
    "    \n",
    "    for col_name in df.columns:\n",
    "\n",
    "        try:\n",
    "            valid_value = data_info.loc[(~data_info['Valido'].isna()) & (data_info['Variable'] == col_name) & (data_info['Nodo'] == node) & (data_info['Tabla'] == tabla), 'Valido'].unique()\n",
    "            invalid_value = data_info.loc[(~data_info['Invalido'].isna()) & (data_info['Variable'] == col_name) & (data_info['Nodo'] == node) & (data_info['Tabla'] == tabla), 'Invalido'].unique()\n",
    "\n",
    "            if len(valid_value) > 0:\n",
    "                invalidos = df.loc[~df[col_name].isin(json.loads(valid_value[0])), col_name].count()\n",
    "                df = df.loc[df[col_name].isin(json.loads(valid_value[0]))]\n",
    "            elif not pd.isna(invalid_value):\n",
    "                invalidos = df.loc[df[col_name] == invalid_value[0], col_name].count()\n",
    "                df = df.loc[df[col_name] != invalid_value[0]]\n",
    "        except:\n",
    "            invalidos = np.nan\n",
    "\n",
    "        # df[col_name] = df[col_name].astype(int) if np.all(df[col_name] == df[col_name].astype(int)) else df[col_name]\n",
    "        col_dtype = df[col_name].dtype\n",
    "        num_of_nulls = df[col_name].isnull().sum()\n",
    "        num_of_non_nulls = df[col_name].notnull().sum()\n",
    "        num_of_distinct_values = df[col_name].nunique()\n",
    "\n",
    "        if is_numeric_dtype(df[col_name]):\n",
    "            min_value = np.round(df[col_name].min(),3)\n",
    "            max_value = np.round(df[col_name].max(),3)\n",
    "            mean_of_values = np.round(df[col_name].dropna().mean(), 3)\n",
    "            q_1 = np.round(np.quantile(df[col_name].dropna(), 0.25), 3)\n",
    "            median_of_values = np.round(df[col_name].dropna().median(), 3)\n",
    "            q_3 = np.round(np.quantile(df[col_name].dropna(), 0.75), 3)\n",
    "            variance_of_values = np.round(np.var(df[col_name].dropna()), 3)\n",
    "        else:\n",
    "            min_value = df[col_name].min()\n",
    "            max_value = df[col_name].max()\n",
    "            mean_of_values = np.nan\n",
    "            q_1 = np.nan\n",
    "            median_of_values = np.nan\n",
    "            q_3 = np.nan\n",
    "            variance_of_values = np.nan\n",
    "        \n",
    "        # if num_of_distinct_values <= 10:\n",
    "        #     distinct_values_counts = df[col_name].value_counts().to_dict()\n",
    "        # else:\n",
    "        #     top_10_values_counts = df[col_name].value_counts().head(10).to_dict()\n",
    "        #     distinct_values_counts = {k: v for k, v in sorted(top_10_values_counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        summary_data.append({\n",
    "            'sistema': system,\n",
    "            'nodo': node,\n",
    "            'tabla': tabla,\n",
    "            'variable': col_name,\n",
    "            'tipo': col_dtype,\n",
    "            'num_de_nulos': num_of_nulls,\n",
    "            'num_de_no_nulos': num_of_non_nulls,\n",
    "            'num_de_val_distintos': num_of_distinct_values,\n",
    "            'num_de_val_invalidos': invalidos,\n",
    "            'min': min_value,\n",
    "            'max': max_value,\n",
    "            'promedio': mean_of_values,\n",
    "            'Q1': q_1,\n",
    "            'mediana': median_of_values,\n",
    "            'Q3': q_3,\n",
    "            'varianza': variance_of_values\n",
    "            # 'distinct_values_counts': distinct_values_counts\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_dataframe(data):\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    # Normalizar los datos JSON\n",
    "    df = pd.json_normalize(data)\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        # Convertir la columna de timestamps a formato de fecha\n",
    "        df[\"Date\"] = df[\"Ts\"].apply(datetime.fromtimestamp)\n",
    "        \n",
    "        if \"Param.Index\" in df.columns:\n",
    "            # Modificar \"Param.Index\" incrementando en 1 y luego rellenando NaN con 0\n",
    "            df.loc[~df[\"Param.Index\"].isna(), \"Param.Index\"] += 1\n",
    "            df[\"Param.Index\"] = df[\"Param.Index\"].fillna(0).astype(int)\n",
    "\n",
    "            # Restructurar los datos con pivot para hacer un solo merge\n",
    "            final_df = df.pivot_table(index=\"Date\", columns=[\"Param.Name\", \"Param.Index\"], values=\"Val\")\n",
    "\n",
    "            # Aplanar las columnas del DataFrame resultante\n",
    "            final_df.columns = [f\"{name}{'_'+str(idx) if idx > 0 else ''}\" for name, idx in final_df.columns]\n",
    "        else:\n",
    "            # Restructurar los datos con pivot para hacer un solo merge\n",
    "            final_df = df.pivot_table(index=\"Date\", columns=[\"Param.Name\"], values=\"Val\")\n",
    "        \n",
    "        # agregar el numero de la tabla\n",
    "        final_df['Tabla'] = df[\"Param.Table\"].unique()[0]\n",
    "\n",
    "        # Resetear el índice para que 'Date' sea una columna normal\n",
    "        final_df.reset_index(inplace=True)\n",
    "    else:\n",
    "        print(\"No hay informacion de telemetria!!\")\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultima_fecha = datetime(2024, 8, 25, 0, 0) # datetime(2023, 3, 16, 0, 0)\n",
    "un_mes_atras = ultima_fecha - timedelta(days=30)\n",
    "timestamp_mes_atras = int(un_mes_atras.timestamp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripcion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodo OBC: 1, Tabla # 4\n",
      "Nodo OBC: 1, Tabla # 92\n",
      "Nodo OBC: 1, Tabla # 93\n",
      "Nodo ADCS: 4, Tabla # 1\n",
      "Nodo ADCS: 4, Tabla # 4\n",
      "Nodo ADCS: 4, Tabla # 5\n",
      "Nodo ADCS: 4, Tabla # 150\n",
      "Nodo ADCS: 4, Tabla # 151\n",
      "Nodo ADCS: 4, Tabla # 152\n",
      "Nodo ADCS: 4, Tabla # 153\n",
      "Nodo ADCS: 4, Tabla # 154\n",
      "Nodo ADCS: 4, Tabla # 156\n",
      "Nodo AX2150: 5, Tabla # 4\n",
      "Nodo P60 DOCK: 6, Tabla # 4\n",
      "Nodo P60 PDU 1: 7, Tabla # 4\n",
      "Nodo P60 PDU 2: 8, Tabla # 4\n",
      "Nodo P60 ACU: 10, Tabla # 4\n",
      "Nodo mddvbs2-control-app: 14, Tabla # 4\n",
      "Nodo mddvbs2-control-app: 14, Tabla # 14\n",
      "Nodo Payload App: 20, Tabla # 8\n",
      "Nodo Payload App: 20, Tabla # 9\n",
      "Nodo Payload App: 20, Tabla # 11\n",
      "Nodo Payload App: 20, Tabla # 12\n",
      "Nodo AFE8250: 23, Tabla # 4\n"
     ]
    }
   ],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\", connectTimeoutMS=30000)\n",
    "\n",
    "# Definir el tamaño del lote\n",
    "batch_size = 1000000\n",
    "final_data_resume = pd.DataFrame()\n",
    "\n",
    "# Se lee el diccionario de datos, el cual contiene los valores validos e invalidos de cada variable\n",
    "dic_datos = pd.read_csv(\"outputs/diccionario_de_datos.csv\")\n",
    "dic_datos.loc[dic_datos['Indice'] >= 0, 'Indice'] += 1\n",
    "dic_datos['Indice'] = dic_datos['Indice'].fillna(0)\n",
    "dic_datos['Variable'] = [row['Variable'] + \"_\" + str(int(row['Indice'])) if row[\"Indice\"] > 0 else row['Variable'] for _, row in dic_datos.iterrows()]\n",
    "\n",
    "loop_over_nodes = dic_datos.groupby(by=['Sistema','Nodo', 'Tabla'], as_index=False)['Indice'].mean().sort_values(by=['Nodo', 'Tabla'])\n",
    "# loop_over_nodes = loop_over_nodes[0:2] # Prueba\n",
    "\n",
    "# Iterar sobre los nodos y procesar cada nodo en lotes\n",
    "for sistema, num_nodo, num_tabla in zip(loop_over_nodes.Sistema, loop_over_nodes.Nodo, loop_over_nodes.Tabla):\n",
    "    skip_count = 0\n",
    "    print(f\"Nodo {sistema}: {num_nodo}, Tabla # {num_tabla}\")\n",
    "    node_data_frames = []  # Lista para almacenar DataFrames de cada lote\n",
    "    expected_columns = None  # Para almacenar las columnas esperadas\n",
    "\n",
    "    while True:\n",
    "        # Realizar la consulta por lotes\n",
    "        node_dict = query_mongodb(client=client, collection='ParamData', query_json={\"Param.Node\": num_nodo, \"Param.Table\": num_tabla}, limit=batch_size, skip=skip_count)\n",
    "        \n",
    "        # Verificar si el lote tiene datos\n",
    "        if not node_dict:\n",
    "            break\n",
    "\n",
    "        # Convertir el lote a DataFrame de Polars y procesarlo\n",
    "        node_df = crear_dataframe(node_dict)\n",
    "        \n",
    "        # Agregar el DataFrame a la lista\n",
    "        node_data_frames.append(node_df)\n",
    "        \n",
    "        # Incrementar el contador de saltos\n",
    "        skip_count += batch_size\n",
    "\n",
    "    # Concatenar todos los DataFrames del nodo en uno solo\n",
    "    if node_data_frames:\n",
    "        combined_node_df = pd.concat(node_data_frames)\n",
    "        \n",
    "        # Ejecutar column_summary una sola vez con el DataFrame combinado\n",
    "        node_summary_df = column_summary(combined_node_df, sistema, num_nodo, num_tabla, dic_datos)\n",
    "\n",
    "        # Concatenar al resultado final\n",
    "        final_data_resume = pd.concat([final_data_resume, node_summary_df])\n",
    "\n",
    "# Cerrar la conexión\n",
    "client.close()\n",
    "final_data_resume.to_csv(\"/outputs/descripcion_de_datos.csv\", float_format=\"%.2f\", encoding=\"utf-8\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
