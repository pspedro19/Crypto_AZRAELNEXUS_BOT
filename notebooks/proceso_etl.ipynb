{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T04:07:54.425283Z",
     "start_time": "2024-11-11T04:07:54.225938Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import fireducks.pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "import polars as pl\n",
    "from typing import List, Dict, Union, Any\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "sys.path.append('/home/sebasav/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/airflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.DAG_ETL import run_etl_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 20:55:26,153 - dags.DAG_ETL - INFO - Processing Node OBC: 1, Table # 4 - run_etl_process:412\n",
      "2024-11-17 20:55:26,157 - dags.DAG_ETL - INFO - Creating MongoDB configuration - extract_data:66\n",
      "2024-11-17 20:55:26,157 - dags.DAG_ETL - INFO - Initializing MongoDB configuration - __init__:24\n",
      "2024-11-17 20:55:26,158 - dags.DAG_ETL - INFO - Establishing MongoDB connection - extract_data:70\n",
      "2024-11-17 20:55:26,159 - dags.DAG_ETL - INFO - Verifying MongoDB connection - extract_data:80\n",
      "2024-11-17 20:55:31,177 - dags.DAG_ETL - ERROR - Data extraction failed with error: mongodb_secondary:27017: [Errno -3] Temporary failure in name resolution (configured timeouts: socketTimeoutMS: 30000.0ms, connectTimeoutMS: 30000.0ms), Timeout: 5.0s, Topology Description: <TopologyDescription id: 673a9e8e3d030cdcca104de4, topology_type: Single, servers: [<ServerDescription ('mongodb_secondary', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('mongodb_secondary:27017: [Errno -3] Temporary failure in name resolution (configured timeouts: socketTimeoutMS: 30000.0ms, connectTimeoutMS: 30000.0ms)')>]> - extract_data:108\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sebasav/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/airflow/dags/DAG_ETL.py\", line 81, in extract_data\n",
      "    client.admin.command(\"ping\")\n",
      "  File \"/home/sebasav/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/_csot.py\", line 119, in csot_wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/sebasav/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/database.py\", line 926, in command\n",
      "    with self._client._conn_for_reads(read_preference, session, operation=command_name) as (\n",
      "  File \"/home/sebasav/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/mongo_client.py\", line 1701, in _conn_for_reads\n",
      "    server = self._select_server(read_preference, session, operation)\n",
      "  File \"/home/sebasav/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/mongo_client.py\", line 1649, in _select_server\n",
      "    server = topology.select_server(\n",
      "  File \"/home/sebasav/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/topology.py\", line 398, in select_server\n",
      "    server = self._select_server(\n",
      "  File \"/home/sebasav/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/topology.py\", line 376, in _select_server\n",
      "    servers = self.select_servers(\n",
      "  File \"/home/sebasav/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/topology.py\", line 283, in select_servers\n",
      "    server_descriptions = self._select_servers_loop(\n",
      "  File \"/home/sebasav/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/topology.py\", line 333, in _select_servers_loop\n",
      "    raise ServerSelectionTimeoutError(\n",
      "pymongo.errors.ServerSelectionTimeoutError: mongodb_secondary:27017: [Errno -3] Temporary failure in name resolution (configured timeouts: socketTimeoutMS: 30000.0ms, connectTimeoutMS: 30000.0ms), Timeout: 5.0s, Topology Description: <TopologyDescription id: 673a9e8e3d030cdcca104de4, topology_type: Single, servers: [<ServerDescription ('mongodb_secondary', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('mongodb_secondary:27017: [Errno -3] Temporary failure in name resolution (configured timeouts: socketTimeoutMS: 30000.0ms, connectTimeoutMS: 30000.0ms)')>]>\n",
      "2024-11-17 20:55:31,178 - dags.DAG_ETL - INFO - Closing MongoDB connection - extract_data:112\n",
      "2024-11-17 20:55:31,179 - dags.DAG_ETL - INFO - MongoDB connection closed successfully - extract_data:114\n",
      "2024-11-17 20:55:31,180 - dags.DAG_ETL - INFO - Error in data processing pipeline: mongodb_secondary:27017: [Errno -3] Temporary failure in name resolution (configured timeouts: socketTimeoutMS: 30000.0ms, connectTimeoutMS: 30000.0ms), Timeout: 5.0s, Topology Description: <TopologyDescription id: 673a9e8e3d030cdcca104de4, topology_type: Single, servers: [<ServerDescription ('mongodb_secondary', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('mongodb_secondary:27017: [Errno -3] Temporary failure in name resolution (configured timeouts: socketTimeoutMS: 30000.0ms, connectTimeoutMS: 30000.0ms)')>]> - run_etl_process:474\n"
     ]
    },
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "mongodb_secondary:27017: [Errno -3] Temporary failure in name resolution (configured timeouts: socketTimeoutMS: 30000.0ms, connectTimeoutMS: 30000.0ms), Timeout: 5.0s, Topology Description: <TopologyDescription id: 673a9e8e3d030cdcca104de4, topology_type: Single, servers: [<ServerDescription ('mongodb_secondary', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('mongodb_secondary:27017: [Errno -3] Temporary failure in name resolution (configured timeouts: socketTimeoutMS: 30000.0ms, connectTimeoutMS: 30000.0ms)')>]>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_etl_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdictionary_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../notebooks/outputs/diccionario_de_datos.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/airflow/dags/DAG_ETL.py:420\u001b[0m, in \u001b[0;36mrun_etl_process\u001b[0;34m(batch_size, dictionary_path, bucket_path)\u001b[0m\n\u001b[1;32m    416\u001b[0m combined_node_df \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;66;03m# Query MongoDB in batches\u001b[39;00m\n\u001b[0;32m--> 420\u001b[0m     node_dict \u001b[38;5;241m=\u001b[39m \u001b[43mextract_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mParamData\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mParam.Node\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mParam.Table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_tabla\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node_dict:\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/airflow/dags/DAG_ETL.py:81\u001b[0m, in \u001b[0;36mextract_data\u001b[0;34m(collection, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Verify connection\u001b[39;00m\n\u001b[1;32m     80\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVerifying MongoDB connection\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mping\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMongoDB connection verified successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Prepare and execute query\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/_csot.py:119\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/database.py:926\u001b[0m, in \u001b[0;36mDatabase.command\u001b[0;34m(self, command, value, check, allowable_errors, read_preference, codec_options, session, comment, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_preference \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    925\u001b[0m     read_preference \u001b[38;5;241m=\u001b[39m (session \u001b[38;5;129;01mand\u001b[39;00m session\u001b[38;5;241m.\u001b[39m_txn_read_preference()) \u001b[38;5;129;01mor\u001b[39;00m ReadPreference\u001b[38;5;241m.\u001b[39mPRIMARY\n\u001b[0;32m--> 926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn_for_reads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommand_name\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[1;32m    927\u001b[0m     connection,\n\u001b[1;32m    928\u001b[0m     read_preference,\n\u001b[1;32m    929\u001b[0m ):\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command(\n\u001b[1;32m    931\u001b[0m         connection,\n\u001b[1;32m    932\u001b[0m         command,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    940\u001b[0m     )\n",
      "File \u001b[0;32m~/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/mongo_client.py:1701\u001b[0m, in \u001b[0;36mMongoClient._conn_for_reads\u001b[0;34m(self, read_preference, session, operation)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_conn_for_reads\u001b[39m(\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1696\u001b[0m     read_preference: _ServerMode,\n\u001b[1;32m   1697\u001b[0m     session: Optional[ClientSession],\n\u001b[1;32m   1698\u001b[0m     operation: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   1699\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ContextManager[\u001b[38;5;28mtuple\u001b[39m[Connection, _ServerMode]]:\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m read_preference \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_preference must not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1701\u001b[0m     server \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn_from_server(read_preference, server, session)\n",
      "File \u001b[0;32m~/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/mongo_client.py:1649\u001b[0m, in \u001b[0;36mMongoClient._select_server\u001b[0;34m(self, server_selector, session, operation, address, deprioritized_servers, operation_id)\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m AutoReconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserver \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m no longer available\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m address)  \u001b[38;5;66;03m# noqa: UP031\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1649\u001b[0m         server \u001b[38;5;241m=\u001b[39m \u001b[43mtopology\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43mserver_selector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m            \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m server\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PyMongoError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# Server selection errors in a transaction are transient.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/topology.py:398\u001b[0m, in \u001b[0;36mTopology.select_server\u001b[0;34m(self, selector, operation, server_selection_timeout, address, deprioritized_servers, operation_id)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_server\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     selector: Callable[[Selection], Selection],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Server:\n\u001b[1;32m    397\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Like select_servers, but choose a random server if several match.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     server \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_selection_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _csot\u001b[38;5;241m.\u001b[39mget_timeout():\n\u001b[1;32m    407\u001b[0m         _csot\u001b[38;5;241m.\u001b[39mset_rtt(server\u001b[38;5;241m.\u001b[39mdescription\u001b[38;5;241m.\u001b[39mmin_round_trip_time)\n",
      "File \u001b[0;32m~/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/topology.py:376\u001b[0m, in \u001b[0;36mTopology._select_server\u001b[0;34m(self, selector, operation, server_selection_timeout, address, deprioritized_servers, operation_id)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select_server\u001b[39m(\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    369\u001b[0m     selector: Callable[[Selection], Selection],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    375\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Server:\n\u001b[0;32m--> 376\u001b[0m     servers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_servers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_selection_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_id\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     servers \u001b[38;5;241m=\u001b[39m _filter_servers(servers, deprioritized_servers)\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(servers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/topology.py:283\u001b[0m, in \u001b[0;36mTopology.select_servers\u001b[0;34m(self, selector, operation, server_selection_timeout, address, operation_id)\u001b[0m\n\u001b[1;32m    280\u001b[0m     server_timeout \u001b[38;5;241m=\u001b[39m server_selection_timeout\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 283\u001b[0m     server_descriptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_servers_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    288\u001b[0m         cast(Server, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_server_by_address(sd\u001b[38;5;241m.\u001b[39maddress)) \u001b[38;5;28;01mfor\u001b[39;00m sd \u001b[38;5;129;01min\u001b[39;00m server_descriptions\n\u001b[1;32m    289\u001b[0m     ]\n",
      "File \u001b[0;32m~/Documentos/FACSAT-2/Git_Repo/DataMiningFACSAT2/.venv/lib/python3.10/site-packages/pymongo/synchronous/topology.py:333\u001b[0m, in \u001b[0;36mTopology._select_servers_loop\u001b[0;34m(self, selector, timeout, operation, operation_id, address)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _SERVER_SELECTION_LOGGER\u001b[38;5;241m.\u001b[39misEnabledFor(logging\u001b[38;5;241m.\u001b[39mDEBUG):\n\u001b[1;32m    323\u001b[0m         _debug_log(\n\u001b[1;32m    324\u001b[0m             _SERVER_SELECTION_LOGGER,\n\u001b[1;32m    325\u001b[0m             message\u001b[38;5;241m=\u001b[39m_ServerSelectionStatusMessage\u001b[38;5;241m.\u001b[39mFAILED,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m             failure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_message(selector),\n\u001b[1;32m    332\u001b[0m         )\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServerSelectionTimeoutError(\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_message(selector)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Timeout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, Topology Description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m     )\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logged_waiting:\n\u001b[1;32m    338\u001b[0m     _debug_log(\n\u001b[1;32m    339\u001b[0m         _SERVER_SELECTION_LOGGER,\n\u001b[1;32m    340\u001b[0m         message\u001b[38;5;241m=\u001b[39m_ServerSelectionStatusMessage\u001b[38;5;241m.\u001b[39mWAITING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m         remainingTimeMS\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()),\n\u001b[1;32m    347\u001b[0m     )\n",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m: mongodb_secondary:27017: [Errno -3] Temporary failure in name resolution (configured timeouts: socketTimeoutMS: 30000.0ms, connectTimeoutMS: 30000.0ms), Timeout: 5.0s, Topology Description: <TopologyDescription id: 673a9e8e3d030cdcca104de4, topology_type: Single, servers: [<ServerDescription ('mongodb_secondary', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('mongodb_secondary:27017: [Errno -3] Temporary failure in name resolution (configured timeouts: socketTimeoutMS: 30000.0ms, connectTimeoutMS: 30000.0ms)')>]>"
     ]
    }
   ],
   "source": [
    "run_etl_process(dictionary_path=\"../notebooks/outputs/diccionario_de_datos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceso de ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicion de funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T04:09:22.293425Z",
     "start_time": "2024-11-11T04:09:22.197673Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_mongodb(client, collection: str = '', query_json: dict = {}, query_project: dict = {}, skip: int = 0, limit: int = 0):\n",
    "    # Define MongoDB parameters\n",
    "    mongo_srv_uri = 'mongodb://localhost:27017/'\n",
    "    mongo_user = ''\n",
    "    mongo_password = ''\n",
    "    mongo_db_name = 'facsat2'\n",
    "    mongo_collection_name = collection\n",
    "\n",
    "    try:\n",
    "        # Access the MongoDB database\n",
    "        db = client[mongo_db_name]\n",
    "        collection = db[mongo_collection_name]\n",
    "\n",
    "        # Perform MongoDB operations\n",
    "        result = collection.find(query_json, query_project).skip(skip).limit(limit) \n",
    "    except Exception as e:\n",
    "        print(\"No fue posible realizar la consulta\\nError:\\n\")\n",
    "        print(e)\n",
    "    \n",
    "    data = list(result) # pl.DataFrame(list(result))\n",
    "    return data\n",
    "\n",
    "def crear_dataframe_polars(\n",
    "    data: List[Dict[str, Any]], \n",
    "    system: str, \n",
    "    table: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a DataFrame from JSON telemetry data using Polars for improved performance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : List[Dict[str, Any]]\n",
    "        JSON data containing telemetry information\n",
    "    system : str\n",
    "        System identifier to be used in column names\n",
    "    table : str\n",
    "        Table identifier to be used in column names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    polars.DataFrame\n",
    "        Processed DataFrame with restructured telemetry data\n",
    "        \n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If input data is empty or required columns are missing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input data\n",
    "        if not data:\n",
    "            raise ValueError(\"Input data is empty\")\n",
    "        \n",
    "        # Create Polars DataFrame from JSON\n",
    "        # df = pl.DataFrame(data)\n",
    "        # Normalizar los datos JSON\n",
    "        df = pl.json_normalize(data)\n",
    "        \n",
    "        if df.is_empty():\n",
    "            raise ValueError(\"DataFrame is empty\")\n",
    "            \n",
    "        # Validate required columns\n",
    "        required_columns = {'Ts', 'Val', 'Param.Name', 'Param.Table'}\n",
    "        missing_columns = required_columns - set(df.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "            \n",
    "        # Convert timestamp to datetime\n",
    "        df = df.with_columns([\n",
    "            pl.col('Ts').cast(pl.Int64).map_elements(datetime.fromtimestamp).alias('Date')\n",
    "        ])\n",
    "        \n",
    "        # Initialize final DataFrame schema\n",
    "        has_index = 'Param.Index' in df.columns\n",
    "        \n",
    "        if has_index:\n",
    "            # Handle indexed parameters\n",
    "            df = df.with_columns([\n",
    "                pl.when(pl.col('Param.Index').is_not_null())\n",
    "                .then(pl.col('Param.Index') + 1)\n",
    "                .otherwise(pl.lit(0))\n",
    "                .alias('Param.Index')\n",
    "                .cast(pl.Int64)\n",
    "            ])\n",
    "            \n",
    "            # Pivot the data\n",
    "            final_df = (\n",
    "                df.pivot(\n",
    "                    values='Val',\n",
    "                    index='Date',\n",
    "                    columns=['Param.Name', 'Param.Index'],\n",
    "                    aggregate_function='first'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Get column names (excluding 'Date')\n",
    "            old_columns = [col for col in final_df.columns if col != 'Date']\n",
    "            \n",
    "            # Create new column names\n",
    "            new_columns = {}\n",
    "            for col in old_columns:\n",
    "                # Parse the string representation of the tuple\n",
    "                # Format is like '{\"mag\",1}' or '{\"mag\",0}'\n",
    "                parts = col.strip('{}').split(',')\n",
    "                param_name = parts[0].strip('\"')  # Remove quotes\n",
    "                index = int(parts[1].strip())     # Convert to int\n",
    "                new_name = (\n",
    "                    f\"{system}{str(table)}_{param_name}\"\n",
    "                    if index == 0 else\n",
    "                    f\"{system}{str(table)}_{param_name}_{str(int(index))}\"\n",
    "                )\n",
    "                new_columns[col] = new_name\n",
    "            \n",
    "        else:\n",
    "            # Handle non-indexed parameters\n",
    "            final_df = (\n",
    "                df.pivot(\n",
    "                    values='Val',\n",
    "                    index='Date',\n",
    "                    columns='Param.Name',\n",
    "                    aggregate_function='first'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Rename columns with system and table identifiers\n",
    "            new_columns = {\n",
    "                col: f\"{system}{str(table)}_{col}\"\n",
    "                for col in final_df.columns\n",
    "                if col != 'Date'\n",
    "            }\n",
    "        \n",
    "        # Apply column renaming\n",
    "        final_df = final_df.rename(new_columns)\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing telemetry data: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(error_msg) from e\n",
    "\n",
    "# def check_columns_missmatch(data_frame):\n",
    "#     # Get a complete set of all unique columns across all dataframes\n",
    "#     all_columns = set()\n",
    "#     for df in data_frame:\n",
    "#         all_columns.update(df.columns)\n",
    "\n",
    "#     # For each DataFrame, add missing columns with null values before concatenation\n",
    "#     aligned_dfs = []\n",
    "#     for df in data_frame:\n",
    "#         missing_cols = list(all_columns - set(df.columns))\n",
    "        \n",
    "#         # Create a new DataFrame with missing columns filled with nulls\n",
    "#         if missing_cols:\n",
    "#             # Create a DataFrame with null values for missing columns\n",
    "#             null_df = pl.DataFrame({\n",
    "#                 col: [None] * len(df) for col in missing_cols\n",
    "#             })\n",
    "#             # Combine with original DataFrame\n",
    "#             aligned_df = pl.concat([df, null_df], how='horizontal')\n",
    "#         else:\n",
    "#             aligned_df = df\n",
    "        \n",
    "#         aligned_dfs.append(aligned_df)\n",
    "\n",
    "#     return aligned_dfs\n",
    "\n",
    "def check_columns_missmatch(dataframes: List[pl.DataFrame]) -> List[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Optimizada versión de la función para alinear columnas y tipos de datos en DataFrames de Polars.\n",
    "    \n",
    "    Args:\n",
    "        dataframes: Lista de DataFrames de Polars a alinear\n",
    "        \n",
    "    Returns:\n",
    "        Lista de DataFrames alineados con tipos de datos consistentes\n",
    "    \"\"\"\n",
    "    if not dataframes:\n",
    "        return []\n",
    "    \n",
    "    # Obtener schema de todas las columnas en un solo paso\n",
    "    schemas = {\n",
    "        col: dtype\n",
    "        for df in dataframes\n",
    "        for col, dtype in df.schema.items()\n",
    "    }\n",
    "    \n",
    "    # Determinar el tipo más compatible para cada columna\n",
    "    final_schema = {}\n",
    "    for col in schemas:\n",
    "        types = [df.schema.get(col) for df in dataframes if col in df.columns]\n",
    "        final_schema[col] = (\n",
    "            pl.Float64 if pl.Int64 in types and pl.Float64 in types\n",
    "            else types[0]\n",
    "        )\n",
    "    \n",
    "    # Función helper para procesar un DataFrame\n",
    "    def align_dataframe(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        # Preparar expresiones para las columnas existentes y nuevas\n",
    "        expressions = []\n",
    "        \n",
    "        # Procesar todas las columnas necesarias\n",
    "        for col, target_type in final_schema.items():\n",
    "            if col in df.columns:\n",
    "                # Si la columna existe y necesita conversión\n",
    "                if df.schema[col] != target_type:\n",
    "                    expressions.append(\n",
    "                        pl.col(col).cast(target_type).alias(col)\n",
    "                    )\n",
    "            else:\n",
    "                # Si la columna no existe, crear con nulls\n",
    "                expressions.append(\n",
    "                    pl.lit(None).cast(target_type).alias(col)\n",
    "                )\n",
    "        \n",
    "        # Si no hay transformaciones necesarias, retornar el DataFrame original\n",
    "        if not expressions:\n",
    "            return df\n",
    "            \n",
    "        # Aplicar todas las transformaciones en una sola operación\n",
    "        return df.with_columns(expressions)\n",
    "    \n",
    "    # Procesar todos los DataFrames usando map\n",
    "    return list(map(align_dataframe, dataframes))\n",
    "\n",
    "\n",
    "def clean_data_with_flexible_references(data_df: pl.DataFrame, ref_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Limpia datos utilizando referencias inválidas y válidas de forma flexible.\n",
    "\n",
    "    Args:\n",
    "        data_df: DataFrame de datos a limpiar.\n",
    "        ref_df: DataFrame de referencias con columnas 'Variable', 'Invalido' y 'Valido'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame limpio.\n",
    "    \"\"\"\n",
    "\n",
    "    # def clean_column(col_name, col_refs):\n",
    "    #     if col_refs.is_empty():  # Check if reference DataFrame is empty\n",
    "    #         return pl.col(col_name)  # Keep the original column\n",
    "\n",
    "    #     invalid_value = col_refs.get_column('Invalido').item() if not col_refs.is_empty() else None\n",
    "    #     valid_values = col_refs.get_column('Valido').item() if not col_refs.is_empty() else None\n",
    "\n",
    "    #     invalid_condition = pl.col(col_name) == invalid_value if invalid_value is not None else pl.lit(False)\n",
    "    #     valid_condition = pl.col(col_name).is_in(valid_values) if valid_values is not None else pl.lit(True)\n",
    "\n",
    "    #     final_condition = ~(invalid_condition | ~valid_condition)\n",
    "\n",
    "    #     return pl.when(final_condition).then(pl.col(col_name)).otherwise(None).alias(col_name)\n",
    "    def clean_column(col_name, col_refs):\n",
    "        if col_refs.is_empty():  # Check if reference DataFrame is empty\n",
    "            return pl.col(col_name)  # Keep the original column\n",
    "\n",
    "        invalid_value = col_refs.get_column('Invalido').item() if not col_refs.is_empty() else None\n",
    "        # valid_values = col_refs.with_columns(pl.col('Valido').str.extract_all(r'(\\d+)').list.eval(pl.element().cast(pl.Int64)).alias('list_column')) if not col_refs.is_empty() else None # col_refs.get_column('Valido').to_list()\n",
    "\n",
    "        # Extract the list of valid values\n",
    "        if not col_refs.is_empty():\n",
    "            valid_values = (col_refs.select(\n",
    "                pl.col('Valido')\n",
    "                .str.extract_all(r'(\\d+)')\n",
    "                .list.eval(pl.element().cast(pl.Int64))\n",
    "            ).item())\n",
    "        else:\n",
    "            valid_values = None\n",
    "\n",
    "        invalid_condition = pl.col(col_name) == invalid_value if invalid_value is not None else pl.lit(False)\n",
    "        valid_condition = pl.col(col_name).is_in(valid_values) if valid_values is not None else pl.lit(True)\n",
    "\n",
    "        final_condition = ~(invalid_condition | ~valid_condition)\n",
    "\n",
    "        return pl.when(final_condition).then(pl.col(col_name)).otherwise(None).alias(col_name)\n",
    "\n",
    "    return data_df.select([\n",
    "        clean_column(col_name, ref_df.filter(pl.col('Variable') == col_name))\n",
    "        for col_name in data_df.columns\n",
    "    ])\n",
    "\n",
    "def remove_full_null_rows(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows from a Polars DataFrame where all columns (except 'Date') contain null values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pl.DataFrame\n",
    "        Input Polars DataFrame\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pl.DataFrame\n",
    "        DataFrame with rows removed where all columns (except 'Date') were null\n",
    "    \"\"\"\n",
    "    # Get all column names except 'Date'\n",
    "    cols_to_check = [col for col in df.columns if col != 'Date']\n",
    "    \n",
    "    # Create a boolean mask for each row where True means the row has at least one non-null value\n",
    "    # in columns other than 'Date'\n",
    "    mask = df.select(\n",
    "        pl.any_horizontal(~pl.col(cols_to_check).is_null())\n",
    "    ).to_series()\n",
    "    \n",
    "    # Filter the DataFrame using the mask\n",
    "    return df.filter(mask)\n",
    "\n",
    "def send_to_mongodb(data_df: pl.DataFrame, client, to_database: str, to_collection: str):\n",
    "    # Convertir el DataFrame a una lista de diccionarios\n",
    "    data_df = data_df.with_columns(\n",
    "        pl.lit(datetime.now()).alias(\"fecha_creacion\"),\n",
    "        pl.lit(None).cast(pl.Datetime).alias(\"fecha_borrado\")\n",
    "    )\n",
    "\n",
    "    data = data_df.to_dicts()\n",
    "\n",
    "    # Conectar a MongoDB\n",
    "    db = client[to_database]\n",
    "    collection = db[to_collection]\n",
    "\n",
    "    # Insertar los datos en MongoDB\n",
    "    collection.insert_many(data)\n",
    "\n",
    "def process_mongodb_data(\n",
    "    mongodb_uri: str = \"mongodb://localhost:27017/\",\n",
    "    batch_size: int = 500000,\n",
    "    dictionary_path: str = \"/outputs/diccionario_de_datos.csv\",\n",
    "    output_path: str = \"param_data_df.csv\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process MongoDB data using Polars for improved performance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mongodb_uri : str\n",
    "        MongoDB connection URI\n",
    "    batch_size : int\n",
    "        Number of documents to process in each batch\n",
    "    dictionary_path : str\n",
    "        Path to the data dictionary CSV file\n",
    "    output_path : str\n",
    "        Path where the final CSV will be saved\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to MongoDB\n",
    "        client = MongoClient(mongodb_uri, connectTimeoutMS=30000)\n",
    "\n",
    "        # Initialize empty DataFrame with Date column\n",
    "        final_dataframe = pl.DataFrame(schema={\"Date\": pl.Datetime})\n",
    "        \n",
    "        # Read and process dictionary data using Polars\n",
    "        dic_datos = pl.read_csv(dictionary_path)\n",
    "        \n",
    "        # Process indices\n",
    "        dic_datos = dic_datos.with_columns([\n",
    "            pl.when(pl.col('Indice') >= 0)\n",
    "            .then(pl.col('Indice') + 1)\n",
    "            .otherwise(pl.lit(0))\n",
    "            .cast(pl.Int64)\n",
    "            .alias('Indice')\n",
    "        ])\n",
    "        \n",
    "        # Create variable names with indices\n",
    "        dic_datos = dic_datos.with_columns([\n",
    "            pl.when(pl.col('Indice') > 0)\n",
    "            .then(pl.col('Sistema') + pl.col('Tabla').cast(pl.String) + \"_\" + pl.col('Variable') + \"_\" + pl.col('Indice').cast(pl.Int64).cast(pl.Utf8))\n",
    "            .otherwise(pl.col('Sistema') + pl.col('Tabla').cast(pl.String) + \"_\" + pl.col('Variable'))\n",
    "            .alias('Variable')\n",
    "        ])\n",
    "        \n",
    "        # Group by Sistema, Nodo, Tabla\n",
    "        loop_over_nodes = (\n",
    "            dic_datos.group_by(['Sistema', 'Nodo', 'Tabla'])\n",
    "            .agg(pl.col('Indice').mean())\n",
    "            .sort(['Nodo', 'Tabla'])\n",
    "        )\n",
    "\n",
    "        # loop_over_nodes = loop_over_nodes.filter(pl.col('Sistema') == \"ADCS\", pl.col('Nodo') == 4, pl.col('Tabla') == 151)\n",
    "        \n",
    "        # Process each node\n",
    "        for row in loop_over_nodes.iter_rows(named=True):\n",
    "            sistema = row['Sistema']\n",
    "            num_nodo = row['Nodo']\n",
    "            num_tabla = row['Tabla']\n",
    "            \n",
    "            print(f\"Processing Node {sistema}: {num_nodo}, Table # {num_tabla}\")\n",
    "            \n",
    "            skip_count = 0\n",
    "            node_data_frames = []\n",
    "            combined_node_df = pl.DataFrame()\n",
    "            \n",
    "            while True:\n",
    "                # Query MongoDB in batches\n",
    "                node_dict = query_mongodb(\n",
    "                    client=client,\n",
    "                    collection='ParamData',\n",
    "                    query_json={\"Param.Node\": num_nodo, \"Param.Table\": num_tabla},\n",
    "                    limit=batch_size,\n",
    "                    skip=skip_count\n",
    "                )\n",
    "                \n",
    "                if not node_dict:\n",
    "                    break\n",
    "                \n",
    "                # Process batch using Polars\n",
    "                try:\n",
    "                    node_df = crear_dataframe_polars(node_dict, sistema, num_tabla)\n",
    "                    node_data_frames.append(node_df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing batch for Node {num_nodo}, Table {num_tabla}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                skip_count += batch_size\n",
    "            \n",
    "            # Combine all batches for this node\n",
    "            if node_data_frames:\n",
    "                # Concatenate all batches for this node\n",
    "                node_data_frames_checked = check_columns_missmatch(node_data_frames)\n",
    "                combined_node_df = pl.concat([df.select(node_data_frames_checked[0].columns) for df in node_data_frames_checked])\n",
    "\n",
    "                # Optimize final dataframe before writing\n",
    "                if not combined_node_df.is_empty():\n",
    "                    # Sort by Date\n",
    "                    combined_node_df = combined_node_df.sort('Date')\n",
    "                    \n",
    "                    # Get all column names except 'Date' and 'Tabla'\n",
    "                    numeric_cols = [col for col in combined_node_df.columns if col not in ['Date', 'Tabla']]\n",
    "\n",
    "                    # Cast those columns to Float32\n",
    "                    combined_node_df = combined_node_df.with_columns([\n",
    "                        pl.col(numeric_cols).cast(pl.Float32)\n",
    "                    ])\n",
    "\n",
    "                    cleaned_node_df = clean_data_with_flexible_references(combined_node_df, dic_datos)\n",
    "                    fully_cleaned_node_df = remove_full_null_rows(cleaned_node_df)\n",
    "                    \n",
    "                    # Write to CSV with optimizations\n",
    "                    # output_path = f\"/outputs/{sistema}_{num_tabla}_collection.csv\"\n",
    "                    # fully_cleaned_node_df.write_csv(\n",
    "                    #     output_path,\n",
    "                    #     float_precision=2,\n",
    "                    #     batch_size=batch_size\n",
    "                    # )\n",
    "\n",
    "                    send_to_mongodb(fully_cleaned_node_df, client, \"etl_data\", f\"{sistema}_{num_tabla}_collection\")\n",
    "\n",
    "                    print(f\"Successfully wrote data to {output_path}\")\n",
    "                    print(f\"Final DataFrame shape: {fully_cleaned_node_df.shape}\")\n",
    "                    print(f\"Memory usage: {fully_cleaned_node_df.estimated_size() / 1024 / 1024:.2f} MB\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data processing pipeline: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        # Close MongoDB connection\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecucion del proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T05:23:11.122399Z",
     "start_time": "2024-11-11T04:09:26.148992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Node OBC: 1, Table # 4\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (60537, 11)\n",
      "Memory usage: 2.77 MB\n",
      "Processing Node OBC: 1, Table # 92\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (63054, 11)\n",
      "Memory usage: 2.96 MB\n",
      "Processing Node OBC: 1, Table # 93\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (63056, 11)\n",
      "Memory usage: 2.96 MB\n",
      "Processing Node ADCS: 4, Table # 1\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (25120, 6)\n",
      "Memory usage: 0.67 MB\n",
      "Processing Node ADCS: 4, Table # 4\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (25120, 12)\n",
      "Memory usage: 1.25 MB\n",
      "Processing Node ADCS: 4, Table # 5\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (25120, 3)\n",
      "Memory usage: 0.38 MB\n",
      "Processing Node ADCS: 4, Table # 150\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (212295, 124)\n",
      "Memory usage: 104.34 MB\n",
      "Processing Node ADCS: 4, Table # 151\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (213543, 33)\n",
      "Memory usage: 28.51 MB\n",
      "Processing Node ADCS: 4, Table # 152\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (141212, 45)\n",
      "Memory usage: 25.52 MB\n",
      "Processing Node ADCS: 4, Table # 153\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (180798, 21)\n",
      "Memory usage: 15.60 MB\n",
      "Processing Node ADCS: 4, Table # 154\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (158465, 23)\n",
      "Memory usage: 14.92 MB\n",
      "Processing Node ADCS: 4, Table # 156\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (23076, 20)\n",
      "Memory usage: 1.85 MB\n",
      "Processing Node AX2150: 5, Table # 4\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (60447, 19)\n",
      "Memory usage: 4.74 MB\n",
      "Processing Node P60 DOCK: 6, Table # 4\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (87784, 95)\n",
      "Memory usage: 33.13 MB\n",
      "Processing Node P60 PDU 1: 7, Table # 4\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (108456, 74)\n",
      "Memory usage: 31.97 MB\n",
      "Processing Node P60 PDU 2: 8, Table # 4\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (92167, 74)\n",
      "Memory usage: 27.17 MB\n",
      "Processing Node P60 ACU: 10, Table # 4\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (35135, 64)\n",
      "Memory usage: 8.98 MB\n",
      "Processing Node mddvbs2-control-app: 14, Table # 4\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (47147, 52)\n",
      "Memory usage: 9.82 MB\n",
      "Processing Node mddvbs2-control-app: 14, Table # 14\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (47536, 24)\n",
      "Memory usage: 4.66 MB\n",
      "Processing Node Payload App: 20, Table # 8\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (38900, 8)\n",
      "Memory usage: 1.34 MB\n",
      "Processing Node Payload App: 20, Table # 9\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (38519, 3)\n",
      "Memory usage: 0.59 MB\n",
      "Processing Node Payload App: 20, Table # 11\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (38327, 51)\n",
      "Memory usage: 7.60 MB\n",
      "Processing Node Payload App: 20, Table # 12\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (37655, 8)\n",
      "Memory usage: 1.29 MB\n",
      "Processing Node AFE8250: 23, Table # 4\n",
      "Successfully wrote data to param_data_df.csv\n",
      "Final DataFrame shape: (43859, 34)\n",
      "Memory usage: 6.02 MB\n"
     ]
    }
   ],
   "source": [
    "curr_dir = os.path.abspath(os.getcwd())\n",
    "target_dict = curr_dir + \"/outputs/diccionario_de_datos.csv\"\n",
    "process_mongodb_data(dictionary_path=target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned data with both invalid and valid references:\n",
      "shape: (8, 4)\n",
      "┌────────────┬──────┬──────┬──────┐\n",
      "│ date       ┆ col1 ┆ col2 ┆ col3 │\n",
      "│ ---        ┆ ---  ┆ ---  ┆ ---  │\n",
      "│ str        ┆ i64  ┆ i64  ┆ f64  │\n",
      "╞════════════╪══════╪══════╪══════╡\n",
      "│ 2024-10-23 ┆ 1    ┆ 9    ┆ 10.0 │\n",
      "│ 2024-10-24 ┆ 2    ┆ 8    ┆ null │\n",
      "│ 2024-10-25 ┆ null ┆ 7    ┆ 30.0 │\n",
      "│ 2024-10-26 ┆ 4    ┆ 6    ┆ 40.0 │\n",
      "│ 2024-10-27 ┆ null ┆ null ┆ 50.0 │\n",
      "│ 2024-10-28 ┆ 7    ┆ 4    ┆ 60.0 │\n",
      "│ 2024-10-29 ┆ 8    ┆ 3    ┆ 70.0 │\n",
      "│ 2024-10-30 ┆ null ┆ 2    ┆ null │\n",
      "└────────────┴──────┴──────┴──────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "def clean_data_with_references(\n",
    "    data_df: pl.DataFrame,\n",
    "    ref_df: pl.DataFrame\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean data using both invalid references and list-based valid references without loops.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_df : pl.DataFrame\n",
    "        DataFrame containing the variables to clean\n",
    "    ref_df : pl.DataFrame\n",
    "        DataFrame containing invalid and valid reference values per variable\n",
    "        Expected columns: 'variable_name', 'invalid_value', 'valid_value'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pl.DataFrame\n",
    "        Cleaned DataFrame with:\n",
    "        - invalid values replaced by null\n",
    "        - values not in valid reference lists replaced by null\n",
    "    \"\"\"\n",
    "    # Create expressions for each column based on reference values\n",
    "    clean_expressions = []\n",
    "    \n",
    "    for col_name in data_df.columns:\n",
    "        # Get reference data for this column\n",
    "        col_refs = ref_df.filter(pl.col('Variable') == col_name)\n",
    "        \n",
    "        # If no references found for this column, keep original\n",
    "        if col_refs.height == 0:\n",
    "            clean_expressions.append(pl.col(col_name))\n",
    "            continue\n",
    "            \n",
    "        # Initialize condition as None\n",
    "        final_condition = None\n",
    "        \n",
    "        # Handle invalid values\n",
    "        invalid_value = col_refs.get_column('Invalido')[0]\n",
    "        if isinstance(invalid_value, (int, float, str)):\n",
    "            invalid_condition = pl.col(col_name).cast(pl.Utf8).eq(str(invalid_value))\n",
    "            final_condition = invalid_condition\n",
    "        \n",
    "        # Handle valid values\n",
    "        valid_values = col_refs.get_column('Valido')[0]\n",
    "        if valid_values is not None:\n",
    "            valid_values = valid_values.to_list()\n",
    "\n",
    "        if isinstance(valid_values, list) and len(valid_values) > 0:\n",
    "            # Convert all values to strings for comparison\n",
    "            valid_values_str = [str(v) for v in valid_values]\n",
    "            valid_condition = ~pl.col(col_name).cast(pl.Utf8).is_in(valid_values_str)\n",
    "            \n",
    "            # Combine with existing condition if any\n",
    "            if final_condition is not None:\n",
    "                final_condition = final_condition | valid_condition\n",
    "            else:\n",
    "                final_condition = valid_condition\n",
    "        \n",
    "        # Create the cleaning expression\n",
    "        if final_condition is not None:\n",
    "            clean_expr = pl.when(final_condition).then(None).otherwise(pl.col(col_name)).alias(col_name)\n",
    "            clean_expressions.append(clean_expr)\n",
    "        else:\n",
    "            clean_expressions.append(pl.col(col_name))\n",
    "    \n",
    "    # Apply all cleaning expressions at once\n",
    "    return data_df.select(clean_expressions)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    data = pl.DataFrame({\n",
    "        'date': ['2024-10-23', '2024-10-24', '2024-10-25', '2024-10-26', '2024-10-27', '2024-10-28', '2024-10-29', '2024-10-30'],\n",
    "        'col1': [1, 2, -999, 4, -999, 7, 8, 9],\n",
    "        'col2': [9, 8, 7, 6, 500, 4, 3, 2],\n",
    "        'col3': [10.0, -888.8, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0]\n",
    "    })\n",
    "    \n",
    "    # Referencias de valores válidos e inválidos\n",
    "    invalid_ref = pl.DataFrame(\n",
    "        {\n",
    "            'Variable': ['col1', 'col2', 'col3'],\n",
    "            'Invalido': [None, 500, None],\n",
    "            'Valido': [\n",
    "                '[4, 2, 1, 7, 8]',\n",
    "                None,\n",
    "                '[10.0, 30.0, 40.0, 50.0, 60.0, 70.0]'\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Clean the data\n",
    "    cleaned_data = clean_data_with_flexible_references(data, invalid_ref) # clean_data_with_references(data, invalid_ref)\n",
    "    print(\"\\nCleaned data with both invalid and valid references:\")\n",
    "    print(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned data with both invalid and valid references:\n",
      "shape: (8, 4)\n",
      "┌────────────┬──────┬──────┬──────┐\n",
      "│ date       ┆ col1 ┆ col2 ┆ col3 │\n",
      "│ ---        ┆ ---  ┆ ---  ┆ ---  │\n",
      "│ str        ┆ i64  ┆ i64  ┆ f64  │\n",
      "╞════════════╪══════╪══════╪══════╡\n",
      "│ 2024-10-23 ┆ 1    ┆ 9    ┆ 10.0 │\n",
      "│ 2024-10-24 ┆ 2    ┆ 8    ┆ null │\n",
      "│ 2024-10-25 ┆ null ┆ 7    ┆ 30.0 │\n",
      "│ 2024-10-26 ┆ 4    ┆ 6    ┆ 40.0 │\n",
      "│ 2024-10-27 ┆ null ┆ null ┆ 50.0 │\n",
      "│ 2024-10-28 ┆ 7    ┆ 4    ┆ 60.0 │\n",
      "│ 2024-10-29 ┆ 8    ┆ 3    ┆ 70.0 │\n",
      "│ 2024-10-30 ┆ null ┆ 2    ┆ null │\n",
      "└────────────┴──────┴──────┴──────┘\n"
     ]
    }
   ],
   "source": [
    "def clean_data_with_flexible_references(data_df: pl.DataFrame, ref_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Limpia datos utilizando referencias inválidas y válidas de forma flexible.\n",
    "\n",
    "    Args:\n",
    "        data_df: DataFrame de datos a limpiar.\n",
    "        ref_df: DataFrame de referencias con columnas 'Variable', 'Invalido' y 'Valido'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame limpio.\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_column(col_name, col_refs):\n",
    "        if col_refs.is_empty():  # Check if reference DataFrame is empty\n",
    "            return pl.col(col_name)  # Keep the original column\n",
    "\n",
    "        invalid_value = col_refs.get_column('Invalido').item() if not col_refs.is_empty() else None\n",
    "        valid_values = col_refs.get_column('Valido').item() if not col_refs.is_empty() else None\n",
    "\n",
    "        invalid_condition = pl.col(col_name) == invalid_value if invalid_value is not None else pl.lit(False)\n",
    "        valid_condition = pl.col(col_name).is_in(valid_values) if valid_values is not None else pl.lit(True)\n",
    "\n",
    "        final_condition = ~(invalid_condition | ~valid_condition)\n",
    "\n",
    "        return pl.when(final_condition).then(pl.col(col_name)).otherwise(None).alias(col_name)\n",
    "\n",
    "    return data_df.select([\n",
    "        clean_column(col_name, ref_df.filter(pl.col('Variable') == col_name))\n",
    "        for col_name in data_df.columns\n",
    "    ])\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    data = pl.DataFrame({\n",
    "        'date': ['2024-10-23', '2024-10-24', '2024-10-25', '2024-10-26', '2024-10-27', '2024-10-28', '2024-10-29', '2024-10-30'],\n",
    "        'col1': [1, 2, -999, 4, -999, 7, 8, 9],\n",
    "        'col2': [9, 8, 7, 6, 500, 4, 3, 2],\n",
    "        'col3': [10.0, -888.8, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0]\n",
    "    })\n",
    "    \n",
    "    # Referencias de valores válidos e inválidos\n",
    "    invalid_ref = pl.DataFrame(\n",
    "        {\n",
    "            'Variable': ['col1', 'col2', 'col3'],\n",
    "            'Invalido': [None, 500, None],\n",
    "            'Valido': [\n",
    "                [4, 2, 1, 7, 8],\n",
    "                None,\n",
    "                [10.0, 30.0, 40.0, 50.0, 60.0, 70.0]\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Clean the data\n",
    "    cleaned_data = clean_data_with_flexible_references(data, invalid_ref)\n",
    "    print(\"\\nCleaned data with both invalid and valid references:\")\n",
    "    print(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obc = query_mongodb('ParamData', {\"Param.Node\": nodos['OBC'], \"Ts\": { \"$gte\": timestamp_mes_atras}}, limit=0)\n",
    "obc_df = crear_dataframe(obc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obc_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns\n",
    "numerical_columns = obc_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Perform univariate analysis on numerical columns\n",
    "for column in numerical_columns:\n",
    "    # For continuous variables\n",
    "    if len(obc_df[column].unique()) > 10:  # Assuming if unique values > 10, consider it continuous\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.histplot(obc_df[column], kde=True)\n",
    "        plt.title(f'Histogram of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "    else:  # For discrete or ordinal variables\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        ax = sns.countplot(x=column, data=obc_df)\n",
    "        plt.title(f'Count of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Annotate each bar with its count\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(format(p.get_height(), '.0f'), \n",
    "                        (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                        ha = 'center', va = 'center', \n",
    "                        xytext = (0, 5), \n",
    "                        textcoords = 'offset points')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento y transformacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in obc_df.columns:\n",
    "    obc_df[col_name] = obc_df[col_name].astype(int) if np.all(obc_df[col_name] == obc_df[col_name].astype(int)) else obc_df[col_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_1 = query_mongodb('ParamData', {\"Param.Node\": 1, \"Ts\": { \"$gte\": timestamp_mes_atras}}, limit=0)\n",
    "node_4 = query_mongodb('ParamData', {\"Param.Node\": 4, \"Ts\": { \"$gte\": timestamp_mes_atras}}, limit=0)\n",
    "node_1_df = crear_dataframe(node_1)\n",
    "node_4_df = crear_dataframe(node_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = node_1_df.merge(node_4_df, on=['Date'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_filtered = result_df.filter(regex='Date|_x|_y|_z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = result_df_filtered[['Date', 'temp_mcu_x', 'temp_mcu_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_filtered = a.dropna(subset=a.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_filtered = a.loc[:, a.std() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "mask = np.triu(np.ones_like(a_filtered.corr(), dtype=np.bool))\n",
    "sns.heatmap(a_filtered.corr(), annot=False, cmap=\"coolwarm\", mask=mask, vmin=-1, vmax=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
